# **Methodology**

When our group chose the topic **Public Attitudes Toward AI in Customer Service**, we knew that a **survey** would be the best way to capture diverse opinions. We wanted to hear from students with different experiences, so we designed a structured yet approachable questionnaire using **Google Forms**. It was quick to distribute and allowed us to automatically collect and organize responses.

We built our survey collaboratively. It had **35 questions** in total, divided into sections that moved logically from background familiarity to specific feelings about AI tools. The attitudinal sections covered **helpfulness**, **convenience**, **trust**, **frustration**, and **preference**. Each statement used a **five-point Likert scale**, which gave us measurable data for comparison and analysis. The final section gathered simple demographic details to help us see whether age, education, or comfort with technology influenced responses.

Our questions were written carefully to avoid bias and confusion. We kept the tone positive and neutral for instance, saying *“I find it easy to communicate my needs when using AI customer service tools”* rather than a negatively loaded alternative. We wanted participants to answer honestly, not defensively.

We shared the survey link randomly among **students from various programs**, not just data analytics majors. Our aim was to keep sampling open and inclusive, so anyone who had interacted with an AI-based customer service system could contribute. We hoped to reach around **100 participants**, which we considered manageable yet statistically meaningful for our analysis.

Data collection ran smoothly thanks to **Google Forms**. Responses were exported directly to a spreadsheet, where we cleaned the data removing incomplete entries and coding categorical responses. We planned our analysis based on what we learned in class: **descriptive statistics** to summarize trends, **Cronbach’s alpha** for reliability, **t-tests** and **ANOVA** for group comparisons, and **correlations** to see how variables like trust and helpfulness related to overall preference (Pallant, 2020).

Because our project was **cross-sectional**, we gathered all responses at one point in time. We knew this design wouldn’t show changes over time, but it captured an accurate picture of current student attitudes. In future, a longitudinal design could explore how perceptions evolve as AI tools become more advanced.

Our teamwork process followed the **Agile mindset** we studied earlier in the module (DataScience-PM, 2023). We set up a **Kanban board** with columns *Backlog*, *In Progress*, *Review*, and *Done* and moved tasks along as we completed them. We met weekly for **stand-up sessions** where each person shared progress, challenges, and priorities. This made it easy to track who was doing what and helped us quickly identify bottlenecks. The structure also encouraged accountability without micromanagement.

After we collected the data, we held a **retrospective reflection session**. This was one of the most valuable parts of the process. We discussed what worked (like dividing tasks early) and what didn’t (like underestimating time for cleaning data). Reflecting helped us adapt our approach for the analysis and report-writing phases. It also mirrored the continuous improvement loop central to Agile projects *inspect and adapt* (SKIB353 Module Materials, 2025).

**Ethical conduct** was essential throughout. We added an opening statement explaining that participation was anonymous and voluntary. We didn’t ask for names, emails, or sensitive data, and we reassured participants that their responses would be used only for academic purposes. This transparency built trust and likely improved data quality.

By the end of the project, our methodology reflected the balance between **academic structure and real-world agility**. The survey design gave us quantifiable, reliable data, while Agile teamwork kept the process flexible and collaborative. It was rewarding to see how ideas from theory like hypothesis testing, reflection, and cross-sectional analysis came alive in practice. Beyond understanding people’s views on AI, we learned how effective research depends not just on tools and tests, but on communication, adaptability, and shared ownership of results.

---

## **References**

- DataScience-PM (2023) *Agile Data Science and Kanban in Data Projects*. Available at: [https://www.datascience-pm.com/agile-data-science/](https://www.datascience-pm.com/agile-data-science/) (Accessed: 3 November 2025).  
- Pallant, J. (2020) *SPSS Survival Manual*. 7th ed. Maidenhead: McGraw-Hill Education.  
- SKIB353 Module Materials (2025) *Skills for Data Analysts: Empirical Research and Agile Practice*. University Moodle resources, Weeks 1–3.
